{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1MZJN0FkKGAP7hxPLB2Fy_G7lOeyJNXJg",
      "authorship_tag": "ABX9TyNsePpaKyDQ6mE6q/D00Exw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce17aa4cf2e4e4b83be60fc61c95125": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7ec1834b56c342f89485e293ae75f7e6",
              "IPY_MODEL_a4ba3e368ffb4c73859602efca17744c",
              "IPY_MODEL_b9ce1a7a14834f21910b757369452311"
            ],
            "layout": "IPY_MODEL_71447e9a06e34646b57161b4fa50e862"
          }
        },
        "7ec1834b56c342f89485e293ae75f7e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26b46a2bc13d414cb32d7fc2d70fb66a",
            "placeholder": "​",
            "style": "IPY_MODEL_035dc476c6a548b395e836824e7908d2",
            "value": "100%"
          }
        },
        "a4ba3e368ffb4c73859602efca17744c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94f164632c5d4d80a5b00cec6679fb40",
            "max": 818322941,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1621125ddf80430c82a882f1fa19691f",
            "value": 818322941
          }
        },
        "b9ce1a7a14834f21910b757369452311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e01d7d25de4f47ee824e8a2d3383e8ec",
            "placeholder": "​",
            "style": "IPY_MODEL_37857847972346caa7c7580258a72be3",
            "value": " 780M/780M [00:50&lt;00:00, 11.0MB/s]"
          }
        },
        "71447e9a06e34646b57161b4fa50e862": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b46a2bc13d414cb32d7fc2d70fb66a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "035dc476c6a548b395e836824e7908d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f164632c5d4d80a5b00cec6679fb40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1621125ddf80430c82a882f1fa19691f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e01d7d25de4f47ee824e8a2d3383e8ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37857847972346caa7c7580258a72be3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clara-lan/Capstone/blob/SVM_YOLO_Object/CapstoneSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use SVM to detect empty space and pre-process images"
      ],
      "metadata": {
        "id": "J93ZoQDqaIYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Import system and dependancies"
      ],
      "metadata": {
        "id": "Nwzzdw3yaTjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5LtTklnf1hEW",
        "outputId": "47e5f29f-0435-4214-9d4f-09de0eb2b9de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting roboflow\n",
            "  Downloading roboflow-0.2.17.tar.gz (25 kB)\n",
            "Collecting certifi==2021.5.30\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting chardet==4.0.0\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[K     |████████████████████████████████| 178 kB 55.4 MB/s \n",
            "\u001b[?25hCollecting cycler==0.10.0\n",
            "  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
            "Requirement already satisfied: glob2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.7)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.21.6)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.1.48 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.23.0)\n",
            "Collecting requests_toolbelt\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Collecting urllib3==1.26.6\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 44.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.64.1)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.3.1->roboflow) (4.1.1)\n",
            "Collecting requests\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 999 kB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.1.1)\n",
            "Building wheels for collected packages: roboflow, wget\n",
            "  Building wheel for roboflow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for roboflow: filename=roboflow-0.2.17-py3-none-any.whl size=31935 sha256=546e54d43a14affc2489cf3c198c5fdb444109e49cf154c612c931f5d84ac7b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/38/3c/b4ac4d8a9d9b44bdcd51f6148ec810b0f05a404e5fed8df48d\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=8d27c00eeceaf655fc77934380e96ae51137a6283bbe414ec880f7f5959eb966\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built roboflow wget\n",
            "Installing collected packages: urllib3, certifi, requests, pyparsing, cycler, wget, requests-toolbelt, python-dotenv, chardet, roboflow\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2022.9.24\n",
            "    Uninstalling certifi-2022.9.24:\n",
            "      Successfully uninstalled certifi-2022.9.24\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.11.0\n",
            "    Uninstalling cycler-0.11.0:\n",
            "      Successfully uninstalled cycler-0.11.0\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 3.0.4\n",
            "    Uninstalling chardet-3.0.4:\n",
            "      Successfully uninstalled chardet-3.0.4\n",
            "Successfully installed certifi-2021.5.30 chardet-4.0.0 cycler-0.10.0 pyparsing-2.4.7 python-dotenv-0.21.0 requests-2.28.1 requests-toolbelt-0.10.1 roboflow-0.2.17 urllib3-1.26.6 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "cycler",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import cv2\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from skimage.feature import hog"
      ],
      "metadata": {
        "id": "Qz5dhKUwauId"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dataset from Roboflow"
      ],
      "metadata": {
        "id": "876UEB-fahHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from skimage import feature"
      ],
      "metadata": {
        "id": "DGizEmtn98UI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL"
      ],
      "metadata": {
        "id": "JJOwwmQWCC9y"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.fixes import sklearn\n",
        "import tensorflow.python.platform\n",
        "from tensorflow.python.platform import gfile"
      ],
      "metadata": {
        "id": "IpgNXMqw-4Hy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Read method1: Load roboflow dataset from Roboflow in tfrecord\"\"\"\n",
        "rf = Roboflow(api_key=\"cKIuGvQRsLbBvFgxNztc\")\n",
        "project = rf.workspace(\"myworkspace-nfnwm\").project(\"projectdb\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EN_dFXHkgpV",
        "outputId": "2b1bf369-2abc-4c7d-9fe0-7dd152a6e32d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = project.version(2).download(\"tfrecord\")"
      ],
      "metadata": {
        "id": "cr4CxtiolaFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Download dataset in YOLOv7 object\"\"\"\n",
        "!pwd\n",
        "yolo_dataset = project.version(2).download(\"yolov7\")"
      ],
      "metadata": {
        "id": "D28vQ2GLkooD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Read images from tf record\"\"\"\n",
        "def extract_images(path,mode):\n",
        "  \"\"\"\n",
        "    Function: extract_images\n",
        "    Input: path - valid image path in tf record\n",
        "           mode - 1 for training, 2 for test\n",
        "    Output: image dict, {key: {image, label}}, key is index\n",
        "    Description: This function extract images and labels from tfrecord, maps them to corresponding index\n",
        "  \"\"\"\n",
        "  images = {}\n",
        "  # Create a description of the features.\n",
        "  features = {\n",
        "      \"image/encoded\": tf.io.FixedLenFeature([], tf.string),\n",
        "      \"image/format\": tf.io.FixedLenFeature([], tf.string),\n",
        "      \"image/height\": tf.io.FixedLenFeature([], tf.int64),\n",
        "      \"image/object/bbox/xmax\": tf.io.VarLenFeature(tf.float32),\n",
        "      \"image/object/bbox/xmin\": tf.io.VarLenFeature(tf.float32),\n",
        "      \"image/object/bbox/ymax\": tf.io.VarLenFeature(tf.float32),\n",
        "      \"image/object/bbox/ymin\": tf.io.VarLenFeature(tf.float32),\n",
        "      \"image/object/class/label\": tf.io.VarLenFeature(tf.int64),\n",
        "      \"image/object/class/text\": tf.io.VarLenFeature(tf.string),\n",
        "      \"image/width\": tf.io.FixedLenFeature([], tf.int64),\n",
        "  }\n",
        "  size = 0\n",
        "  idx = 0\n",
        "  if mode == 1:\n",
        "    size = 400\n",
        "  elif mode == 2:\n",
        "    size = 100\n",
        "  train_dataset = tf.data.TFRecordDataset(path)\n",
        "  for raw_record in train_dataset.take(size):\n",
        "    sample = tf.io.parse_single_example(raw_record, features)\n",
        "    image = tf.image.decode_image(sample['image/encoded'], dtype=tf.float32) \n",
        "    label = sample['image/object/class/label']\n",
        "    images[idx] = [image, label]\n",
        "    idx+=1\n",
        "  return images\n",
        "# path =  '/content/Capstone-3/train/empty-shelf-space.tfrecord'\n",
        "# extract_images(path, 1)"
      ],
      "metadata": {
        "id": "lOfRNPtyQi9u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Read images from YOLOv7\"\"\"\n",
        "\n",
        "def extract_images_from_yolov7(path,mode):\n",
        "  \"\"\"\n",
        "    Function: extract_images\n",
        "    Input: path - valid image path in tf record\n",
        "           mode - 1 for training, 2 for test\n",
        "    Output: image dict, {key: {image, label}}, key is index, image is the images name, label is the name of label file\n",
        "  \"\"\"\n",
        "  image_labels_dict = {}\n",
        "  images_path = path+\"/images\"\n",
        "  labels_path = path+'/labels'\n",
        "  images = [f for f in os.listdir(images_path)]\n",
        "  labels = [f for f in os.listdir(labels_path)]\n",
        "  images.sort()\n",
        "  labels.sort()\n",
        "  for i in range(len(images)):\n",
        "    image_labels_dict[i] = [images[i], labels[i]]\n",
        "    # print(images[i])\n",
        "    # # print(\"tuple: \",images[i], \"====tuple:\", labels[i])\n",
        "    # return\n",
        "\n",
        "  return image_labels_dict\n",
        "\n",
        "# extract_images_from_yolov7('/content/ProjectDB-2/test',1)"
      ],
      "metadata": {
        "id": "AeInO3ark6ul"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step2: Preprocess data"
      ],
      "metadata": {
        "id": "HI6gpfcDapC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Convert Tensor to image\n",
        "  Show Image\n",
        "\"\"\"\n",
        "def show_image_from_tensor(tensor):\n",
        "    tensor = tensor*255\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "    if np.ndim(tensor)>3:\n",
        "        assert tensor.shape[0] == 1\n",
        "        tensor = tensor[0]\n",
        "        img_arr = PIL.Image.fromarray(tensor)\n",
        "        plt.imshow(img_arr, interpolation='nearest')\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "mGTD6oXq-Mok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dataset = tf.data.TFRecordDataset('/content/Empty-shelf-detection-15/test/Empty-space.tfrecord')\n",
        "# valid_dataset = tf.data.TFRecordDataset('/content/Empty-shelf-detection-15/valid/Empty-space.tfrecord')\n",
        "train_labels = '/content/ProjectDB-2/train/empty-shelf-space_label_map.pbtxt'\n",
        "# valid_labels = '/content/Empty-shelf-detection-15/valid/Empty-space_label_map.pbtxt'"
      ],
      "metadata": {
        "id": "1CpXZtaDPG9G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step3: Train SVM\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "GeQM7_yAZniY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datagen(mode):\n",
        "    \"\"\"\n",
        "    Function: datagen \n",
        "    \n",
        "    Input: \n",
        "        mode - 1 denotes train data ; 2 denotes test data\n",
        "    \n",
        "    Output: Train/Test data and labels depending on mode value\n",
        "    \n",
        "    Description: This function computes HOG features for each image in the Dataset/train or Dataset/test folder, assigns label to the descriptor vector of the image and returns the final train/test data and labels matrices used for feeding the SVM in training phase or predicting the label of test data.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "    ind = 0\n",
        "    if mode == 1:\n",
        "        path = '/content/ProjectDB-2/train/empty-shelf-space.tfrecord'\n",
        "    elif mode == 2:\n",
        "        path = '/content/ProjectDB-2/valid/empty-shelf-space.tfrecord'\n",
        "    # read image\n",
        "    images = extract_images(path, mode)\n",
        "    for key in images.keys():\n",
        "        # compute HOG features\n",
        "        img, label = images[key][0], images[key][1]\n",
        "        # print(type(img))\n",
        "        des, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(4, 4), block_norm= 'L2',visualize=True)\n",
        "        # print(\"des:\",des)\n",
        "        # print(hog_image)\n",
        "        # print(\"des: \", des)\n",
        "        \"\"\"Read labels\"\"\"\n",
        "        # cv2_imshow(hog_image)\n",
        "        # append descriptor and label to train/test data, labels\n",
        "        label = tf.sparse.to_dense(label).numpy().tolist()\n",
        "        if(len(label) > 0):\n",
        "          X.append(des)\n",
        "          y.append(label)\n",
        "    return X, y\n",
        "# datagen(2)\n",
        "# x,y = datagen(1)\n",
        "# print(\"y: \", y)\n"
      ],
      "metadata": {
        "id": "1iD7Wrf-ZuPp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\"\"\"Read labels from .txt file\"\"\"\n",
        "def read_labels(filename):\n",
        "  labels = [x.split(' ') for x in open(filename).readlines()]\n",
        "  for label in labels:\n",
        "    label[-1].rstrip()\n",
        "  return labels\n"
      ],
      "metadata": {
        "id": "Ceie-MOraGDo"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "from random import randrange\n",
        "def datagen_yolov7(mode):\n",
        "    \"\"\"\n",
        "    Function: datagen_yolov7 \n",
        "    \n",
        "    Input: \n",
        "        mode - 1 denotes train data ; 2 denotes test data\n",
        "    \n",
        "    Output: Train/Test data and labels depending on mode value\n",
        "    \n",
        "    Description: This function computes HOG features for each image in the Dataset/train or Dataset/test folder, assigns label to the descriptor vector of the image and returns the final train/test data and labels matrices used for feeding the SVM in training phase or predicting the label of test data.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    X = []\n",
        "    y = []\n",
        "    ind = 0\n",
        "    if mode == 1:\n",
        "        path = '/content/ProjectDB-2/train'\n",
        "    elif mode == 2:\n",
        "        path = '/content/ProjectDB-2/valid'\n",
        "    # read image\n",
        "    image_labels_dict = extract_images_from_yolov7(path, mode)\n",
        "    for key in image_labels_dict.keys():\n",
        "        # compute HOG features\n",
        "        img, label = image_labels_dict[key][0], image_labels_dict[key][1]\n",
        "        # print(img)\n",
        "        image = cv2.imread(path+'/images/'+img)\n",
        "        # cv2_imshow(image)\n",
        "        # return\n",
        "        img = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "        img = cv2.resize(img,(128,128))\n",
        "\n",
        "        # print(type(img))\n",
        "        des, hog_image = hog(img, orientations=8, pixels_per_cell=(16, 16), cells_per_block=(4, 4), block_norm= 'L2',visualize=True)\n",
        "        # print(\"des:\",des)\n",
        "        # print(hog_image)\n",
        "        # print(\"des: \", des)\n",
        "        \"\"\"Read labels\"\"\"\n",
        "        # cv2_imshow(hog_image)\n",
        "        # append descriptor and label to train/test data, labels\n",
        "        X.append(des)\n",
        "        # print(des)\n",
        "        label = read_labels(path+'/labels/'+label)\n",
        "        #store only one label\n",
        "        y.append(label[randrange(len(label))])\n",
        "    return X, y\n",
        "\n",
        "# datagen_yolov7(2)"
      ],
      "metadata": {
        "id": "TybcVsqxqhSB"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def train_svm():\n",
        "#     # list of training and test files\n",
        "#     # call 'datagen' function to get training and testing data & labels\n",
        "#     mlb = MultiLabelBinarizer()\n",
        "#     Xtrain, ytrain = datagen(1)\n",
        "#     Xtest, ytest = datagen(2)\n",
        "\n",
        "#     # print(\"ytrain: \", ytrain)\n",
        "#     # convert matrices to numpy array for fast computation\n",
        "#     Xtrain_arr = np.array(Xtrain)\n",
        "#     # convert labels to 1d array to enabel sklearn\n",
        "#     ytrain = [item[0] for item in ytrain]\n",
        "#     # print(\"x: \", len(Xtrain))\n",
        "#     # print(\"ytrain: \", ytrain)\n",
        "#     Xtest_arr = np.array(Xtest)\n",
        "#     ytest = [item[0] for item in ytest]\n",
        "\n",
        "#     # training phase: SVM , fit model to training data ------------------------------\n",
        "#     clf = svm.SVC(kernel = 'linear')\n",
        "#     clf.fit(Xtrain_arr, ytrain)\n",
        "#     # predict labels for test data\n",
        "#     ypred = clf.predict(Xtest_arr)\n",
        "    \n",
        "#     # compute accuracy\n",
        "#     accuracy = accuracy_score(ytest, ypred) * 100\n",
        "#     print(\"\\nAccuracy: %.2f\" % accuracy + \"%\")\n",
        "# start_time = time.time()\n",
        "# train_svm()\n",
        "# print('Execution time: %.2f' % (time.time() - start_time) + ' seconds\\n')"
      ],
      "metadata": {
        "id": "0VvYlhGxnFrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_svm_with_yolo():\n",
        "    # list of training and test files\n",
        "    # call 'datagen' function to get training and testing data & labels\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    Xtrain, ytrain = datagen_yolov7(1)\n",
        "    Xtest, ytest = datagen_yolov7(2)\n",
        "\n",
        "    # print(\"ytrain: \", ytrain)\n",
        "    # convert matrices to numpy array for fast computation\n",
        "    Xtrain_arr = np.array(Xtrain)\n",
        "    # convert labels to 1d array to enabel sklearn\n",
        "    ytrain = [item[0] for item in ytrain]\n",
        "    # print(\"x: \", len(Xtrain))\n",
        "    # print(\"ytrain: \", ytrain)\n",
        "    \n",
        "\n",
        "    Xtest_arr = np.array(Xtest)\n",
        "    ytest = [np.reshape(y[0], -1) for y in ytest]\n",
        "\n",
        "    # training phase: SVM , fit model to training data ------------------------------\n",
        "    clf = svm.SVC(kernel = 'linear')\n",
        "    clf.fit(Xtrain_arr, ytrain)\n",
        "    # predict labels for test data\n",
        "    ypred = clf.predict(Xtest_arr)\n",
        "    \n",
        "    # compute accuracy\n",
        "    accuracy = accuracy_score(ytest, ypred) * 100\n",
        "    print(\"\\nAccuracy: %.2f\" % accuracy + \"%\")\n",
        "    \n",
        "start_time = time.time()\n",
        "train_svm_with_yolo()\n",
        "print('Execution time: %.2f' % (time.time() - start_time) + ' seconds\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpod4cUYhTdn",
        "outputId": "89748963-1ed7-41c9-c444-e90aa303643f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy: 53.57%\n",
            "Execution time: 9.53 seconds\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train with YOLO\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "t01hZ95f3wcj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/WongKinYiu/yolov7\n",
        "# %cd yolov7\n",
        "# !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "u8x0qzA9Pdoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGW7gP6yE_fl",
        "outputId": "5326a0c7-a662-4748-e6fb-6341b954d472"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6bL03VPEnLB",
        "outputId": "1d5d2c1f-4dbd-43d2-8bdb-bd8d2d946c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 14442, done.\u001b[K\n",
            "remote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14442 (delta 3), reused 2 (delta 0), pack-reused 14430\u001b[K\n",
            "Receiving objects: 100% (14442/14442), 13.44 MiB | 12.18 MiB/s, done.\n",
            "Resolving deltas: 100% (9980/9980), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRZOXWHoEp89",
        "outputId": "6a992f62-dbd7-4c5b-90c5-43911b890b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCIM0NqJMmB5",
        "outputId": "cf7fb7cf-af1d-4214-b255-620cebdd8ef3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:yolov5:YOLOv5 🚀 v6.2-211-g32a9218 Python-3.7.15 torch-1.8.1+cu101 CPU\n",
            "YOLOv5 🚀 v6.2-211-g32a9218 Python-3.7.15 torch-1.8.1+cu101 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ✅ (2 CPUs, 12.7 GB RAM, 30.2/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Train with YOLOv5\"\"\"\n",
        "rf = Roboflow(api_key=\"cKIuGvQRsLbBvFgxNztc\")\n",
        "project = rf.workspace(\"myworkspace-nfnwm\").project(\"projectdb\")\n",
        "dataset = project.version(2).download(\"yolov5\")"
      ],
      "metadata": {
        "id": "Ceg-ZkkP70P0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d609b477-05e9-4207-87dd-03468e0f7075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in ProjectDB-2 to yolov5pytorch: 100% [41723010 / 41723010] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to ProjectDB-2 in yolov5pytorch:: 100%|██████████| 1144/1144 [00:01<00:00, 1094.04it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPShxy2QJfUP",
        "outputId": "0b85e968-dfab-445e-9423-3e5a44e84cfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 🚀 v6.2-215-g575055c Python-3.7.15 torch-1.8.1+cu101 CPU\n",
            "\n",
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5s.pt to yolov5s.pt...\n",
            "100% 14.1M/14.1M [00:00<00:00, 15.6MB/s]\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "image 1/2 /content/yolov7/yolov5/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 382.5ms\n",
            "image 2/2 /content/yolov7/yolov5/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 247.6ms\n",
            "Speed: 4.5ms pre-process, 315.1ms inference, 21.8ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download COCO val\n",
        "torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n",
        "!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "1ce17aa4cf2e4e4b83be60fc61c95125",
            "7ec1834b56c342f89485e293ae75f7e6",
            "a4ba3e368ffb4c73859602efca17744c",
            "b9ce1a7a14834f21910b757369452311",
            "71447e9a06e34646b57161b4fa50e862",
            "26b46a2bc13d414cb32d7fc2d70fb66a",
            "035dc476c6a548b395e836824e7908d2",
            "94f164632c5d4d80a5b00cec6679fb40",
            "1621125ddf80430c82a882f1fa19691f",
            "e01d7d25de4f47ee824e8a2d3383e8ec",
            "37857847972346caa7c7580258a72be3"
          ]
        },
        "id": "VEyJG8KxPXBx",
        "outputId": "626c2879-5bb1-4791-e0a2-c21d79ba1519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/780M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ce17aa4cf2e4e4b83be60fc61c95125"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate YOLOv5s on COCO val\n",
        "!python val.py --weights yolov5s.pt --data /content/yolov7/yolov5/data/coco.yaml --img 640 --half"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DGXGuO7M6N4",
        "outputId": "89734830-c4b4-47f3-8b17-a1d2ff05f009"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov7/yolov5/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
            "YOLOv5 🚀 v6.2-215-g575055c Python-3.7.15 torch-1.8.1+cu101 CPU\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '/content/yolov7/yolov5/datasets/coco/val2017.cache' images and labels... 4952 found, 48 missing, 0 empty, 0 corrupt: 100% 5000/5000 [00:00<?, ?it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/157 [00:00<?, ?it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"val.py\", line 406, in <module>\n",
            "    main(opt)\n",
            "  File \"val.py\", line 379, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 27, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"val.py\", line 209, in run\n",
            "    preds, train_out = model(im) if compute_loss else (model(im, augment=augment), None)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/yolov7/yolov5/yolov5/models/common.py\", line 515, in forward\n",
            "    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/yolov7/yolov5/yolov5/models/yolo.py\", line 209, in forward\n",
            "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n",
            "  File \"/content/yolov7/yolov5/yolov5/models/yolo.py\", line 121, in _forward_once\n",
            "    x = m(x)  # run\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/content/yolov7/yolov5/yolov5/models/common.py\", line 60, in forward_fuse\n",
            "    return self.act(self.conv(x))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
            "    result = self.forward(*input, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 399, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\", line 396, in _conv_forward\n",
            "    self.padding, self.dilation, self.groups)\n",
            "RuntimeError: \"unfolded2d_copy\" not implemented for 'Half'\n"
          ]
        }
      ]
    }
  ]
}